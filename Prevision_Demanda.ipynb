{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparativa de las técnicas de modelado de la previsión de la demanda para dimensionar el stock en el almacén.\n",
    "\n",
    "\n",
    "### Universitat Oberta de Catalunya - Ciència de Dades\n",
    "### Jordi Puiggròs Cuadras - jpuiggrosc@uoc.edu\n",
    "\n",
    "#### En este notebook se procederá a la obtención, preparación, análisis estadístico y visual del juego de datos para el justificar la memória del proyecto. Así mismo, se realizaran los modelos y se evaluará la calidad de estos:\n",
    "\n",
    "<ol>\n",
    "<li>Preparación del set de datos</li>\n",
    "<li>Modelado</li>\n",
    "<li>Evaluación de la calidad</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparación del set de datos\n",
    "#### Se cargan los datos a partir de los ficheros csv y se preparan para su análisis estadístico y visual. El objetivo es tener un único dataframe final, con los datos preparados, transformados y que puedan ser usados por los distintos modelos de ML que se quieren aplicar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importem llibreries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Cargamos los ficheros\n",
    "#### Existen 4 ficheros: Ventas, Calendario, Promociones y Stocks. Se usaran como columnas indexadoras la fecha y los sku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importem els datasets\n",
    "dsVentas = pd.read_csv('.\\DATA\\Venta.csv',  sep = ';' )\n",
    "dsCalendari = pd.read_csv('.\\DATA\\Calendari.csv',  sep = ';')\n",
    "dsPromos = pd.read_csv('.\\DATA\\Promos.csv',  sep = ';')\n",
    "dsStock = pd.read_csv('.\\DATA\\Stock.csv',  sep = ';')\n",
    "\n",
    "# Calendari: dates en format date\n",
    "dsCalendari.insert(0, \"Data\",pd.to_datetime(dsCalendari['fecha'],format='%Y%m%d'), True)\n",
    "dsCalendari = dsCalendari.drop('fecha', 1) #0-> rows, 1->cols\n",
    "\n",
    "# Ventes: dates en format date\n",
    "dsVentas.insert(0, \"Data\",pd.to_datetime(dsVentas['fecha'],format='%Y%m%d'), True)\n",
    "dsVentas = dsVentas.drop('fecha', 1) #0-> rows, 1->cols\n",
    "\n",
    "# Promos: dates en format date\n",
    "dsPromos.insert(0, \"Data_ini\",pd.to_datetime(dsPromos['fecha ini'],format='%Y%m%d'), True)\n",
    "dsPromos.insert(1, \"Data_fin\",pd.to_datetime(dsPromos['fecha fin'],format='%Y%m%d'), True)\n",
    "dsPromos = dsPromos.drop('fecha ini', 1) #0-> rows, 1->cols\n",
    "dsPromos = dsPromos.drop('fecha fin', 1) #0-> rows, 1->cols\n",
    "\n",
    "# Stock: dates en format date\n",
    "dsStock.insert(0, \"Data\",pd.to_datetime(dsStock['fecha'],format='%Y%m%d'), True)\n",
    "dsStock = dsStock.drop('fecha', 1) #0-> rows, 1->cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1.- Ventas\")\n",
    "print(dsVentas.describe())\n",
    "\n",
    "print(\"2.- Calendario\")\n",
    "print(dsCalendari.describe())\n",
    "\n",
    "print(\"3.- Promociones\")\n",
    "print(dsPromos.describe())\n",
    "\n",
    "print(\"4.- Stocks\")\n",
    "print(dsStock.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionem el fitxer del calendari amb el de ventes\n",
    "dfCalendariVentas = pd.merge(dsCalendari, dsVentas, on=['sku','Data'], how='left')\n",
    "dfCalendariVentas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Fusionamos los ficheros\n",
    "#### El objetivo final de este apartado es conseguir un único dataframe con todos los datos útiles para realizar el análisis y el modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparem el fitxer de promocions per fusionar-lo\n",
    "out = pd.merge(dfCalendariVentas,dsPromos, on='sku', how='left')\n",
    "m = out['Data'].between(out['Data_ini'], out['Data_fin'])\n",
    "Promocions = out[m]\n",
    "#Font: https://tutorialmeta.com/question/is-there-a-way-to-merge-on-interval-index-and-another-column-value-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionem les promocions amb el dataset acumulatiu\n",
    "dfCalendariVentasPromocion = pd.merge(dfCalendariVentas, Promocions, on=['sku','Data'], how='left')\n",
    "dfCalendariVentasPromocion\n",
    "\n",
    "# Creem una nova la columna: \"Promoción\" amb 0 o 1 segons correspongui\n",
    "dfCalendariVentasPromocion.insert(5, \"Promocion\",np.where(dfCalendariVentasPromocion.Data_ini.isnull(), 0, 1), True)\n",
    "\n",
    "# Eliminem les columnes sobrants\n",
    "dfCalendariVentasPromocion = dfCalendariVentasPromocion.drop('bolOpen_y', 1) #0-> rows, 1->cols\n",
    "dfCalendariVentasPromocion = dfCalendariVentasPromocion.drop('bolHoliday_y', 1) #0-> rows, 1->cols\n",
    "dfCalendariVentasPromocion = dfCalendariVentasPromocion.drop('udsVenta_y', 1) #0-> rows, 1->cols\n",
    "\n",
    "# Renombrem columnes\n",
    "dfCalendariVentasPromocion = dfCalendariVentasPromocion.rename(columns={\"bolOpen_x\": \"bolOpen\", \"bolHoliday_x\": \"bolHoliday\", \"udsVenta_x\":\"udsVenta\"})\n",
    "\n",
    "dfCalendariVentasPromocion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afegim els valors del dataset Stock\n",
    "dfCalendariVentasPromocionStock = pd.merge(dfCalendariVentasPromocion, dsStock, on=['sku','Data'], how='left')\n",
    "dfCalendariVentasPromocionStock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Transformación del dataset\n",
    "#### Una vez contamos con un único dataframe, lo trabajaremos para transformarlo en un dataframe útil y sin ruido de fondo. Transformando los valores NaN, quitando las fechas de la serie temporal sin valores (tanto por arriba como por abajo), y reconstruiendo algunos datos basándonos en alguna reglas de calidad básicas que hemos extraido a partir de una primera exploración de los ficheros (por ejemplo, reconstrucción de las ventas cuando el stock sea zero como detallamos en los siguientes apartados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valors de ventes NA substituïm per 0\n",
    "dfCalendariVentasPromocionStock['udsVenta'] = dfCalendariVentasPromocionStock['udsVenta'].fillna(0)\n",
    "\n",
    "# Ordenem i mostrem\n",
    "dfCalendariVentasPromocionStock.sort_values(by=['Data','sku'], inplace=True)\n",
    "dfCalendariVentasPromocionStock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mirem quines són la primera i la última data amb ventes\n",
    "dfMaxMin = dfCalendariVentasPromocionStock.loc[dfCalendariVentasPromocionStock['udsVenta'] > 0]\n",
    "\n",
    "datamax = dfMaxMin['Data'].max()\n",
    "datamin = dfMaxMin['Data'].min()\n",
    "\n",
    "# Filtrem el dataset per agafar només dades amb ventes\n",
    "df = dfCalendariVentasPromocionStock.loc[(dfCalendariVentasPromocionStock['Data'] >= datamin) & (dfCalendariVentasPromocionStock['Data'] <= datamax)]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis y tratamiento de atípicos\n",
    "\n",
    "Por la pandemia, detectamos periodos de ventas a zero, que normalizaremos con un valor medio\n",
    "Análisis de datos atípicos (datos fuera de los límites de control, gráficos box plot...)\n",
    "Análisis de periodos atípicos: durante la pandemia y confinamiento, la venta se fue a cero (qué hacer con esas semanas? lo mejor es reemplazar por datos promedio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reglas de Calidad: \n",
    "#Cuando bolOpen=0 y bolHoliday=0 -> Las ventas siempre son 0\n",
    "df[(df['bolOpen']==0) & (df['bolHoliday']==0) & (df['udsVenta']>0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quan Stock=0 i les ventes=0, però no estem en la situació anterior: s'han de reconstruïr\n",
    "#df[(df['udsStock']==0)  & (df['udsVenta']==0) & ((df['bolOpen']!=0) | (df['bolHoliday']!=0))]\n",
    "df[(df['udsVenta']<=0) & ((df['bolOpen']!=0) | (df['bolHoliday']!=0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolem per a cada un dels 50 SKU\n",
    "#myList=range(1,51)\n",
    "#for i in myList:\n",
    "#    df[df['sku'] == i] = df[df['sku'] == i].interpolate(method ='linear', limit_direction ='forward') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasos per reconstruïr les ventes:\n",
    "\n",
    "# En primer lugar seleccionamos los registros a reconstruir:\n",
    "#mask = ((df['udsStock']==0)  & (df['udsVenta']==0) & ((df['bolOpen']!=0) | (df['bolHoliday']!=0)))\n",
    "mask = ((df['udsVenta']<=0) & ((df['bolOpen']!=0) | (df['bolHoliday']!=0)))\n",
    "\n",
    "myList=range(1,51)\n",
    "for i in myList:\n",
    "    aux = df[df['sku']==i]\n",
    "    mean = aux['udsVenta'].mean()\n",
    "    df.loc[mask & (df['sku']==i),'udsVenta']= mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existeixen varias formas de reconstruir las ventas (los métodos por interpolación obtienen mejores resultados (1)):\n",
    "#dfReconstruido = df[['Data','sku','udsVenta']]\n",
    "#dfReconstruido = dfReconstruido.interpolate();\n",
    "\n",
    "# Mezclamos los valores interpolados en un único dataset\n",
    "#dfReconstruido = pd.merge(df, dfReconstruido, on=['sku','Data'], how='left')\n",
    "\n",
    "# Comprobamos que efectivamente hemos reconstruido los valores de venta con Stock=0\n",
    "#dfReconstruido[dfReconstruido['udsStock']==0 ]\n",
    "\n",
    "# Guardamos un dataset nuevo con todas las variables que queremos usar:\n",
    "\n",
    "#df2 = dfReconstruido[['Data','sku','bolOpen','bolHoliday','Promocion','udsStock','udsVenta_y']]\n",
    "\n",
    "# Renombrem columna\n",
    "#df2 = df2.rename(columns={\"udsVenta_y\": \"udsVenta\"})\n",
    "#df2\n",
    "df = df[['Data','sku','bolOpen','bolHoliday','Promocion','udsStock','udsVenta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Font: \n",
    "(1) Medium. Imputing the Time-Series Using Python. url: https://drnesr.medium.com/filling-gaps-of-a-time-series-using-python-d4bfddd8c460"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Análisis estadístico\n",
    "#### El objetivo es realizar un primer análisis del dataset que hemos preparado en los apartados anteriores\n",
    "\n",
    "##### Vamos a comenzar obteniendo un histograma de frecuencias para conocer la distribución histórica que han tenido las ventas. En este histograma también vamos a añadir la función de densidad de probabilidad (FDP) de una distribución normal. Esto nos permitirá saber si la distribución histórica de los retornos se ajusta, o no, a una distribución normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df2\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Dibujamos el histograma de frecuencias\n",
    "fig,ax = plt.subplots(figsize=(16,5),dpi=100)\n",
    "\n",
    "sns.set(color_codes = True)\n",
    "ax = sns.distplot(df['udsVenta'], bins=100, kde=False, fit=stats.norm, color='green')\n",
    "\n",
    "# Obtenemos los parámetros ajustados de la distribución normal utilizados por SNS\n",
    "(mu, sigma) = stats.norm.fit(df['udsVenta'])\n",
    "\n",
    "# Configuramos la gráfica\n",
    "plt.title('Distribución de las unidades demandadas en el periodo analizado', fontsize = 16)\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.legend([\"Distribución normal. fit ($\\mu=${0:.2g}, $\\sigma=${1:.2f})\".format(mu,sigma),\"Distribución udsVenta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aquí podemos ver que nuestra distribución de udsVenta tiene cierta similitud con una distribución normal, sin embargo, esta no llega a ajustarse perfectamente. En realidad, las series de udsVenta que nos encontramos rara vez, si no nunca, se ajustan perfectamente a una distribución normal. En general tienden a mostrar valores extremos que se desvían de su media con una probabilidad más alta de lo que se espera en una distribución normal. Lo cual produce distribuciones con colas largas y por ende mayor probabilidad de sufrir riesgo de cola (Tail Risk).\n",
    "\n",
    "##### A continuación vamos a obtener la estadística descriptiva de usdVenta. Aquí hemos añadido los estadísticos que considero más importantes. Dentro del código encontraremos comentarios que nos indican para qué sirven las distintas instrucciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos promedio, desviación típica, máximo, mínimo y número de datos analizados para el valor a predecir udsVenta:\n",
    "print('> Media:', '%.6s' % (df['udsVenta'].mean()))\n",
    "print('> Desviación Típica:', '%.6s' % (df['udsVenta'].std(ddof=1)))\n",
    "print('> Mínimo valor:', '%.6s' % ( df['udsVenta'].min()))\n",
    "print('> Máximo valor:', '%.6s' % ( df['udsVenta'].max()))\n",
    "print('> Registros analizados:', '%.6s' % (df['udsVenta'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['udsVenta'])\n",
    "plt.figure(figsize=(16,5), dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadimos el dia de la semana i el mes del año\n",
    "\n",
    "#Análisis de correlación de variables (tipo correlation matrix).\n",
    "import seaborn as sn\n",
    "df['DayWeek'] = df['Data'].dt.weekday\n",
    "df['Month'] = df['Data'].dt.month\n",
    "dfMatrix = pd.DataFrame(df,columns=['sku','bolOpen','bolHoliday','Promocion', 'udsVenta','DayWeek','Month'])\n",
    "\n",
    "\n",
    "corrMatrix = dfMatrix.corr()\n",
    "sn.heatmap(corrMatrix, annot=True)\n",
    "plt.figure(figsize=(16,5), dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot:\n",
    "\n",
    "sns.set()\n",
    "cols = ['sku','bolOpen','bolHoliday','Promocion', 'udsVenta','DayWeek','Month']\n",
    "sns.pairplot(df[cols], size = 2.5)\n",
    "plt.figure(figsize=(16,5), dpi=100)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrama de cajas sku/udsVenta:\n",
    "\n",
    "var = 'sku'\n",
    "data = pd.concat([df['udsVenta'], df[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(16, 5), dpi=100)\n",
    "fig = sns.boxplot(x=var, y=\"udsVenta\", data=data)\n",
    "fig.axis(ymin=0, ymax=200);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_out = df.copy()\n",
    "    outliers = ~df_out[col_name].between(fence_low, fence_high, inclusive=False)\n",
    "    df_out.loc[outliers, col_name] = df_out.loc[~outliers, col_name].mean()\n",
    "    return df_out\n",
    "\n",
    "appended_data = []\n",
    "myList=range(1,51)\n",
    "for i in myList:\n",
    "    data = replace_outlier(df[df['sku'] == i], 'udsVenta')\n",
    "    appended_data.append(data[data['sku'] == i])\n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SinOutliers = pd.concat(appended_data)\n",
    "df_SinOutliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_SinOutliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrama de cajas sku/udsVenta:\n",
    "\n",
    "var = 'sku'\n",
    "data = pd.concat([df['udsVenta'], df[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(16, 5), dpi=100)\n",
    "fig = sns.boxplot(x=var, y=\"udsVenta\", data=data)\n",
    "fig.axis(ymin=0, ymax=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Análisis visual\n",
    "#### El objetivo es realizar un análisis visual del dataset que hemos preparado en los apartados anteriores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparamos una función per crear las gràficas\n",
    "\n",
    "def plot_df(df,x,y1,y2, title=\"Titol\", xlabel='X', y1label='Y', y2label='Y', dpi=100):  \n",
    "    fig,ax = plt.subplots(figsize=(16,5),dpi=dpi)\n",
    "    ax.plot(x, y1, color='tab:red')\n",
    "    ax.set_ylabel(y1label,color=\"red\",fontsize=14)\n",
    "    \n",
    "    # twin object for two different y-axis on the sample plot\n",
    "    ax2=plt.twinx()\n",
    "    ax2.plot(x, y2, color='tab:blue')\n",
    "    ax2.set_ylabel(y2label,color=\"blue\",fontsize=14)\n",
    "   \n",
    "    plt.gca().set(title=title, xlabel=xlabel) \n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grafiquem les séries sobreposant l'Stock para cada uno de los SKUS:\n",
    "by_label = df.groupby('sku')\n",
    "for name, group in by_label:\n",
    "    plot_df(df, x=group['Data'], y1=group['udsVenta'], y2=group['udsStock'], title='sku:'+str(name), xlabel='Data', y1label='Unitats Venudes', y2label='Unitats Stock')\n",
    "#plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelado\n",
    "\n",
    "#### A continuación usaremos dos modelos tradicionales y dos modelos de ML para realizar predicciones forecasting a partir del juego de datos preparado en los apartados anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las columnas que queremos usar en los modelos:\n",
    "df = df[['Data','sku','bolOpen','bolHoliday','Promocion', 'udsVenta','DayWeek','Month']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Modelo 1: Exponential Smoothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "def calcular_mape(actual, pred): \n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    c = np.divide((actual - pred),  actual, out=np.zeros_like((actual - pred)), where= actual!=0)\n",
    "    return np.nanmean(np.abs(c)) * 100\n",
    "    \n",
    "def Predict_SimpleExpSmoothing(df, sku, alpha):\n",
    "    \n",
    "    # Split Train Test\n",
    "    total_size=len(df)\n",
    "    split = 10392 / 11856\n",
    "    train_size=math.floor(split*total_size)\n",
    "\n",
    "    # Test y entrenamiento\n",
    "    train=df.head(train_size)\n",
    "    test=df.tail(len(df) - train_size)\n",
    "    \n",
    "    # Create prediction table\n",
    "    y_hat = test.copy()\n",
    "    fit = SimpleExpSmoothing(np.asarray(train['udsVenta'])).fit(smoothing_level=alpha,optimized=False)\n",
    "    y_hat['SES'] = fit.forecast(len(test))\n",
    "\n",
    "    # Metriques de qualitat , initialization_method='estimated'\n",
    "    rmse = sqrt(mean_squared_error(test.udsVenta, y_hat.SES))\n",
    "    mape = calcular_mape(test.udsVenta,y_hat.SES)\n",
    "    \n",
    "    # Plotting data\n",
    "    plt.figure(figsize=(12,8), dpi=100)\n",
    "    plt.plot(train.Data, train['udsVenta'], label='Train')\n",
    "    plt.plot(test.Data,test['udsVenta'], label='Test')\n",
    "    plt.plot(y_hat.Data,y_hat['SES'], label='SES')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"Simple Exponential Smoothing (SES)\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Medidas a Mostrar\n",
    "    print(\"RMSE (sku:\" + str(sku) +\" alpha:\" + str(alpha) +\" ) = \" + str(rmse) + \" MAPE = \" + str(mape))\n",
    "    \n",
    "    return rmse, mape\n",
    "\n",
    "# Font: \n",
    "# https://github.com/tristanga/Machine-Learning/blob/master/Time%20Series%20Forecasting/Simple%20Exponential%20Smoothing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos la previsión con SE usando distintos niveles de smoothing_level\n",
    "Results_SE = pd.DataFrame(columns=['sku','rmse02', 'rmse06','rmse08', 'mape02', 'mape06','mape08'])\n",
    "myList=range(1,51)\n",
    "\n",
    "for i in myList:\n",
    "    #print(df[df['sku'] == i])\n",
    "    rmse_02, mape02 = Predict_SimpleExpSmoothing(df[df['sku'] == i], i, 0.2)\n",
    "    rmse_06, mape06 = Predict_SimpleExpSmoothing(df[df['sku'] == i], i, 0.6)\n",
    "    rmse_08, mape08 = Predict_SimpleExpSmoothing(df[df['sku'] == i], i, 0.8)\n",
    "    Results_SE = Results_SE.append({'sku': str(i), 'rmse02': rmse_02,'rmse06': rmse_06,'rmse08': rmse_08, 'mape02': mape02, 'mape06': mape06, 'mape08': mape08}, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agafem el millor resultat de tots\n",
    "\n",
    "Results_SE['rmse'] = Results_SE[['rmse02','rmse06','rmse08']].min(axis=1)\n",
    "Results_SE['mape'] = Results_SE[['mape02','mape06','mape08']].max(axis=1)\n",
    "Results_SE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Modelo 2: Moving Averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "def Predict_ARMA(df, sku):\n",
    "    \n",
    "    # Split Train Test\n",
    "    total_size=len(df)\n",
    "    split = 10392 / 11856\n",
    "    train_size=math.floor(split*total_size)\n",
    "\n",
    "    # Test y entrenamiento\n",
    "    train=df.head(train_size)\n",
    "    test=df.tail(len(df) - train_size)\n",
    "    \n",
    "    # Create prediction table\n",
    "    y_hat = test.copy()\n",
    "    model_fitted = ARMA(np.asarray(train['udsVenta']), order=(0, 1) ).fit()\n",
    "    #y_hat['ARMA'] = fit.forecast(len(test))\n",
    "\n",
    "#model_fitted.summary()\n",
    "\n",
    "    y_hat['ARMA'] = model_fitted.predict(start=len(train), end=len(train) + len(test)-1, dynamic= True)\n",
    "    \n",
    "    # Metriques de qualitat\n",
    "    rmse = sqrt(mean_squared_error(test.udsVenta, y_hat.ARMA))\n",
    "    mape = calcular_mape(test.udsVenta,y_hat.ARMA)\n",
    "        \n",
    "    # Plotting data\n",
    "    plt.figure(figsize=(12,8), dpi=100)\n",
    "    plt.plot(train.Data, train['udsVenta'], label='Train')\n",
    "    plt.plot(test.Data,test['udsVenta'], label='Test')\n",
    "    plt.plot(y_hat.Data,y_hat['ARMA'], label='ARMA')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"ARMA\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Medidas a Mostrar\n",
    "    print(\"RMSE (sku:\" + str(sku)+\" ) = \"+ str(rmse) + \" MAPE = \" + str(mape))\n",
    "    \n",
    "    return rmse, mape\n",
    "\n",
    "# https://www.projectpro.io/recipes/forecast-moving-averages-for-time-series\n",
    "#https://puneet166.medium.com/time-series-forecasting-how-to-predict-future-data-using-arma-arima-and-sarima-model-8bd20597cc7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos la previsión con SE usando distintos niveles de smoothing_level\n",
    "Results_ARMA = pd.DataFrame(columns=['sku','rmse'])\n",
    "myList=range(1,51)\n",
    "\n",
    "for i in myList:\n",
    "    #print(df[df['sku'] == i])\n",
    "    rmse, mape = Predict_ARMA(df[df['sku'] == i], i)\n",
    "    Results_ARMA = Results_ARMA.append({'sku': str(i), 'rmse': rmse, 'mape': mape}, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_ARMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Modelo 3: SVM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def Predict_SVR(df, sku):\n",
    "\n",
    "    # Split Train Test\n",
    "    total_size=len(df)\n",
    "    split = 10392 / 11856\n",
    "    train_size=math.floor(split*total_size)\n",
    "\n",
    "    # Test y entrenamiento\n",
    "    train=df.head(train_size)\n",
    "    test=df.tail(len(df) - train_size)\n",
    "\n",
    "    y_hat = test.copy()\n",
    "    \n",
    "    #sc_X = StandardScaler()\n",
    "    #sc_y = StandardScaler()\n",
    "    sc_X = preprocessing.MinMaxScaler()\n",
    "    sc_y = preprocessing.MinMaxScaler()\n",
    "    \n",
    "    X_train = sc_X.fit_transform(train[[\"bolOpen\",\"bolHoliday\",\"Promocion\",'DayWeek']])\n",
    "    y_train = sc_y.fit_transform(train[[\"udsVenta\"]])\n",
    "    X_test  = sc_X.fit_transform(test[[\"bolOpen\",\"bolHoliday\",\"Promocion\",'DayWeek']])\n",
    "    y_test  = sc_y.fit_transform(test[[\"udsVenta\"]])\n",
    "\n",
    "    \n",
    "    param_grid = {\"C\": np.linspace(10**(-2),10**3,100),\n",
    "             'gamma': np.linspace(0.0001,1,20)}\n",
    "    mod = SVR(epsilon = 0.1,kernel='rbf')\n",
    "    model = GridSearchCV(estimator = mod, param_grid = param_grid,\n",
    "                                   scoring = \"neg_mean_squared_error\",verbose = 0)\n",
    "\n",
    "    best_model = model.fit(X_train, y_train.ravel())\n",
    "\n",
    "\n",
    "    #prediction\n",
    "    #predicted_tr = model.predict(X_train)\n",
    "    predicted = model.predict(X_test)\n",
    "\n",
    "    # inverse_transform because prediction is done on scaled inputs\n",
    "    #predicted_tr = sc_y.inverse_transform(predicted_tr)\n",
    "    y_hat['SVR'] = sc_y.inverse_transform(predicted.reshape(-1,1))\n",
    "\n",
    "    # Metriques de qualitat\n",
    "    rmse = sqrt(mean_squared_error(test.udsVenta, y_hat.SVR))\n",
    "    mape = calcular_mape(test.udsVenta,y_hat.SVR)\n",
    "    \n",
    "    # Plotting data\n",
    "    plt.figure(figsize=(12,8), dpi=100)\n",
    "    plt.plot(train.Data, train['udsVenta'], label='Train')\n",
    "    plt.plot(test.Data,test['udsVenta'], label='Test')\n",
    "    plt.plot(y_hat.Data,y_hat['SVR'], label='SVR')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"SVR\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"RMSE (sku:\" + str(sku)+\" ) = \"+ str(rmse) + \" MAPE = \" + str(mape))\n",
    "    print(best_model.best_params_)\n",
    "    \n",
    "    return rmse,mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos la previsión con SRV\n",
    "Results_SVR = pd.DataFrame(columns=['sku','rmse'])\n",
    "myList=range(1,51)\n",
    "\n",
    "for i in myList:\n",
    "    #print(df[df['sku'] == i])\n",
    "    rmse, mape = Predict_SVR(df[df['sku'] == i], i)\n",
    "    Results_SVR = Results_SVR.append({'sku': str(i), 'rmse': rmse, 'mape': mape}, ignore_index=True)\n",
    "\n",
    "#Font: https://medium.com/pursuitnotes/support-vector-regression-in-6-steps-with-python-c4569acd062d\n",
    "#https://www.analyticsvidhya.com/blog/2020/03/support-vector-regression-tutorial-for-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Modelo 4: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "\n",
    "def Predict_LSTM(df, sku):\n",
    "\n",
    "    # Split Train Test\n",
    "    total_size=len(df)\n",
    "    split = 10392 / 11856\n",
    "    train_size=math.floor(split*total_size)\n",
    "\n",
    "    # Test y entrenamiento\n",
    "    train=df.head(train_size)\n",
    "    test=df.tail(len(df) - train_size)\n",
    "\n",
    "    y_hat = test.copy()\n",
    "    \n",
    "    sc_X = StandardScaler()\n",
    "    sc_y = StandardScaler()\n",
    "\n",
    "    X_train = sc_X.fit_transform(train[[\"bolOpen\",\"bolHoliday\",\"Promocion\",'DayWeek']])\n",
    "    y_train = sc_y.fit_transform(train[[\"udsVenta\"]])\n",
    "    X_test  = sc_X.fit_transform(test[[\"bolOpen\",\"bolHoliday\",\"Promocion\",'DayWeek']])\n",
    "    y_test  = sc_y.fit_transform(test[[\"udsVenta\"]])\n",
    "\n",
    "    np.random.seed(42)\n",
    "    #tf.random.set_seed(42)\n",
    "\n",
    "    lstm_model = keras.models.Sequential([\n",
    "        keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "        keras.layers.LSTM(20, return_sequences=True),   ###\n",
    "        keras.layers.LSTM(20),\n",
    "        keras.layers.Dense(1)   #### Note - Dense layer with one output ( One day prediction )\n",
    "    ])\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(lr=0.0005)\n",
    "    lstm_model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    lstm_model.summary()\n",
    "\n",
    "    #### Early stop the training if there is no improvement in val_loss for 5 epochs. Save the best model based on val_loss.\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    mc = keras.callbacks.ModelCheckpoint('best_model_lstm.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "    history_lstm = lstm_model.fit(X_train, y_train, epochs=80, batch_size=32,\n",
    "                                  validation_data=(X_test, y_test), callbacks=[early_stopping, mc])\n",
    "    lstm_model = keras.models.load_model('best_model_lstm.h5')\n",
    "\n",
    "    predicted = lstm_model.predict(X_test)\n",
    "\n",
    "    # inverse_transform because prediction is done on scaled inputs\n",
    "    y_hat['LSTM'] = sc_y.inverse_transform(predicted)\n",
    "\n",
    "    # Metriques de qualitat\n",
    "    rmse = sqrt(mean_squared_error(test.udsVenta, y_hat.LSTM))\n",
    "    mape = calcular_mape(test.udsVenta,y_hat.LSTM)\n",
    "\n",
    "    # Plotting data\n",
    "    plt.figure(figsize=(12,8), dpi=100)\n",
    "    plt.plot(train.Data, train['udsVenta'], label='Train')\n",
    "    plt.plot(test.Data,test['udsVenta'], label='Test')\n",
    "    plt.plot(y_hat.Data,y_hat['LSTM'], label='LSTM')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"LSTM\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"RMSE (sku:\" + str(sku)+\" ) = \"+ str(rmse) + \" MAPE = \" + str(mape))\n",
    "    #print(best_model.best_params_)\n",
    "    \n",
    "    return rmse, mape\n",
    "\n",
    "#Font: https://github.com/raja-surya/Time-Series-RNN/blob/main/Web-Traffic-Time-Series-Forecasting-RNN-LSTM.ipynb\n",
    "#https://medium.com/geekculture/time-series-forecast-using-deep-learning-adef5753ec85\n",
    "# https://stackoverflow.com/questions/53890194/time-series-forcasting-with-svr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos la previsión con SRV\n",
    "Results_LSTM = pd.DataFrame(columns=['sku','rmse'])\n",
    "myList=range(1,51)\n",
    "\n",
    "for i in myList:\n",
    "    #print(df[df['sku'] == i])\n",
    "    rmse, mape = Predict_LSTM(df[df['sku'] == i], i)\n",
    "    Results_LSTM = Results_LSTM.append({'sku': str(i), 'rmse': rmse, 'mape': mape}, ignore_index=True)\n",
    "\n",
    "#Font: https://medium.com/geekculture/time-series-forecast-using-deep-learning-adef5753ec85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluación de la calidad de los modelos\n",
    "\n",
    "#### A continuación usaremos los modelos creados y las previsiones hechas, para evaluar la calidad de los distintos modelos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Results1 = Results_SE[['sku','rmse','mape']]\n",
    "Results1 = Results1.rename(columns={\"rmse\": \"rmse_SE\", \"mape\":\"mape_SE\"})\n",
    "\n",
    "Results2 = Results_ARMA[['sku','rmse','mape']]\n",
    "Results2 = Results2.rename(columns={\"rmse\": \"rmse_ARMA\", \"mape\":\"mape_ARMA\"})\n",
    "\n",
    "Results3 = Results_SVR[['sku','rmse','mape']]\n",
    "Results3 = Results3.rename(columns={\"rmse\": \"rmse_SVR\", \"mape\":\"mape_SVR\"})\n",
    "\n",
    "Results4 = Results_LSTM[['sku','rmse','mape']]\n",
    "Results4 = Results4.rename(columns={\"rmse\": \"rmse_LSTM\", \"mape\":\"mape_LSTM\"})\n",
    "\n",
    "Results = pd.merge(Results1, Results2, on=['sku'], how='left')\n",
    "Results = pd.merge(Results, Results3, on=['sku'], how='left')\n",
    "Results = pd.merge(Results, Results4, on=['sku'], how='left')\n",
    "\n",
    "Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# RSME\n",
    "se = Results[['rmse_SE']].mean()\n",
    "arma = Results[['rmse_ARMA']].mean()\n",
    "svr = Results[['rmse_SVR']].mean()\n",
    "lstm = Results[['rmse_LSTM']].mean()\n",
    "\n",
    "data = [se, arma, svr, lstm]\n",
    "labels = ['se','arma','svr','lstm']\n",
    "list = pd.DataFrame({'x':[labels], 'y':[data],}, columns=['x','y'])\n",
    "p = sns.barplot(x=labels, y=data, data=list, ci=None).set(title='RMSE')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valores medio:\n",
    "\n",
    "print(\"RMSE (valor medio SE) = \"+ str(se))\n",
    "\n",
    "print(\"RMSE (valor medio ARMA) = \"+ str(arma)) \n",
    "\n",
    "print(\"RMSE (valor medio SVR) = \"+ str(svr)) \n",
    "\n",
    "print(\"RMSE (valor medio LSTM) = \"+ str(lstm)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPE\n",
    "se = Results[['mape_SE']].mean()\n",
    "arma = Results[['mape_ARMA']].mean()\n",
    "svr = Results[['mape_SVR']].mean()\n",
    "lstm = Results[['mape_LSTM']].mean()\n",
    "\n",
    "data = [se, arma, svr, lstm]\n",
    "labels = ['se','arma','svr','lstm']\n",
    "\n",
    "list = pd.DataFrame({'x':[labels], 'y':[data],}, columns=['x','y'])\n",
    "p = sns.barplot(x=labels, y=data, data=list, ci=None).set(title='MAPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valores medio:\n",
    "\n",
    "print(\"RMSE (valor medio SE) = \"+ str(se))\n",
    "\n",
    "print(\"RMSE (valor medio ARMA) = \"+ str(arma)) \n",
    "\n",
    "print(\"RMSE (valor medio SVR) = \"+ str(svr)) \n",
    "\n",
    "print(\"RMSE (valor medio LSTM) = \"+ str(lstm)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
